{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of two CNN\n",
    "- Tadhg Ryan 21310408\n",
    "- Szymon Szulc 21323208\n",
    "\n",
    "##### Code executes to end with no errors\n",
    "\n",
    "## Resources:\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T16:26:22.178353Z",
     "start_time": "2024-11-19T16:26:21.121715Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU devices:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "dataset_path = kagglehub.dataset_download(\"muratkokludataset/rice-image-dataset\") + \"\\\\Rice_Image_Dataset\"\n",
    "\n",
    "print(\"Path to dataset files:\", dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "IMG_HEIGHT = 250\n",
    "IMG_WIDTH = 250\n",
    "K = 3\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "MAX_SIZE_DATASET = 1000\n",
    "NUM_OF_BATCHES = MAX_SIZE_DATASET // BATCH_SIZE\n",
    "RESIZED_SIZE = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data in\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create the full dataset (without splitting for validation)\n",
    "train_dataset, test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    color_mode='grayscale',  # Load labels in grayscale\n",
    "    shuffle=True,\n",
    "    validation_split=0.004,\n",
    "    subset=\"both\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Get class names for later use\n",
    "class_names = train_dataset.class_names\n",
    "\n",
    "train_dataset = train_dataset.take(NUM_OF_BATCHES)\n",
    "print(f\"Taking {NUM_OF_BATCHES} batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "class_counts = Counter()\n",
    "\n",
    "for images, labels in train_dataset:\n",
    "    class_counts.update([class_names[label] for label in labels.numpy()])\n",
    "\n",
    "print(f\"Count: {class_counts}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(class_counts.keys(), class_counts.values(), color='skyblue')\n",
    "plt.title(\"Total Images in Each Class\")\n",
    "plt.xlabel(\"Class Names\")\n",
    "plt.ylabel(\"Number of Images\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define the data augmentation pipeline\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\"), \t\t# Flip horizontally\n",
    "    layers.RandomZoom(-0.2, 0.2),        \t\t\t\t# Zoom in on the image\n",
    "    layers.RandomRotation(0.2),          \t\t\t\t# Randomly rotate image\n",
    "    layers.RandomBrightness(factor=(-0.2, 0.2)),\n",
    "])\n",
    "data_scaling = tf.keras.Sequential([\n",
    "    layers.Resizing(RESIZED_SIZE, RESIZED_SIZE),\t\t# Resize to desired dimensions\n",
    "])\n",
    "data_normalisation = tf.keras.Sequential([\n",
    "    layers.Rescaling(1./255),\n",
    "])\n",
    "\n",
    "# Define functions to apply the augmentation\n",
    "def augment_image(image, label):\n",
    "    image = data_augmentation(image, training=True)\n",
    "    return image, label\n",
    "\n",
    "def scale_image(image, label):\n",
    "    image = data_scaling(image, training=True)\n",
    "    return image, label\n",
    "\n",
    "def normalise_image(image, label):\n",
    "    image = data_normalisation(image)\n",
    "    return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "num_images = 9  # Number of images to display\n",
    "\n",
    "# Create lists to store images and augmented images\n",
    "original_images = []\n",
    "augmented_images = []\n",
    "labels_list = []\n",
    "\n",
    "# Iterate through the dataset and collect images and their augmented versions\n",
    "i = 0\n",
    "for image_batch, label_batch in train_dataset:\n",
    "    if i >= num_images:\n",
    "        break\n",
    "    original_images.append(image_batch)  # Save original image\n",
    "    augmented_image = augment_image(image_batch, None)[0]\n",
    "    augmented_image = scale_image(augmented_image, None)[0]\n",
    "    augmented_image = normalise_image(augmented_image, None)[0]\n",
    "    augmented_images.append(augmented_image)  # Save augmented image\n",
    "    labels_list.append(label_batch)\n",
    "    i += BATCH_SIZE\n",
    "\n",
    "# Concatenate the batches into single arrays\n",
    "original_images_array = tf.concat(original_images, axis=0)\n",
    "augmented_images_array = tf.concat(augmented_images, axis=0)\n",
    "labels_array = tf.concat(labels_list, axis=0)\n",
    "\n",
    "# Select the first `num_images` images and labels to display\n",
    "original_images_to_display = original_images_array[:num_images]\n",
    "augmented_images_to_display = augmented_images_array[:num_images]\n",
    "labels_to_display = labels_array[:num_images]\n",
    "\n",
    "# Plot the images in a grid\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "for i in range(num_images):\n",
    "    # Plot original images\n",
    "    ax = plt.subplot(3, 6, 2 * i + 1)  # Adjust for both original and augmented\n",
    "    plt.imshow(original_images_to_display[i].numpy().squeeze(), cmap='gray')\n",
    "    plt.title(f\"Original - {class_names[labels_to_display[i].numpy()]}\")  # Show class name\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Plot augmented images\n",
    "    ax = plt.subplot(3, 6, 2 * i + 2)\n",
    "    plt.imshow(augmented_images_to_display[i].numpy().squeeze(), cmap='gray')\n",
    "    plt.title(f\"Augmented - {class_names[labels_to_display[i].numpy()]}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "augmented_train_dataset         = train_dataset.map(augment_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "scale_augmented_train_dataset   = augmented_train_dataset.map(scale_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_dataset                   = scale_augmented_train_dataset.map(normalise_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "scale_test_dataset              = test_dataset.map(scale_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_dataset                    = scale_test_dataset.map(normalise_image, num_parallel_calls=tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model 1\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def makeInceptionV3(input_shape=(224, 224, 1), num_classes=5, dropout_rate=0.5):\n",
    "\tmodel = models.Sequential([])\n",
    "\tmodel.add(layers.Input(shape=input_shape))\n",
    "\tmodel.add(layers.Resizing(224, 224))\n",
    "\tmodel.add(layers.Lambda(lambda x: tf.repeat(x, 3, axis=-1)))\n",
    "\n",
    "\tinceptionV3 = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "\tfor layer in inceptionV3.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\n",
    "\tmodel.add(inceptionV3)\n",
    " \n",
    "\t# Fully connected layers\n",
    "\tmodel.add(layers.Flatten())\n",
    "\tmodel.add(layers.Dense(128, activation='relu'))\n",
    "\tmodel.add(layers.BatchNormalization())\n",
    "\tmodel.add(layers.Dropout(dropout_rate))\n",
    "\tmodel.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\treturn model, inceptionV3\n",
    "\n",
    "makeInceptionV3()[0].summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ResNet50\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "\n",
    "def makeResNet50(input_shape=(224, 224, 1), num_classes=5, dropout_rate=0.5):\n",
    "    base_model = ResNet50(\n",
    "        include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3)\n",
    "    )\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(input_shape))\n",
    "    model.add(layers.Resizing(224, 224))\n",
    "    model.add(layers.Lambda(lambda x: tf.repeat(x, 3, axis=-1)))\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.add(base_model)\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation=\"relu\"))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "    return model, base_model\n",
    "\n",
    "makeResNet50()[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16 define\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import VGG19\n",
    "\n",
    "def makeVGG19(input_shape=(224, 224, 1), num_classes=5, dropout_rate=0.5):\n",
    "\tmodel = models.Sequential()\n",
    "\n",
    "\tmodel.add(layers.Input(input_shape))\n",
    "\tmodel.add(layers.Resizing(224, 224))\n",
    "\tmodel.add(layers.Lambda(lambda x: tf.repeat(x, 3, axis=-1)))\n",
    "\n",
    "\tvgg19 = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "\tfor layer in vgg19.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\n",
    "\tmodel.add(vgg19)\n",
    " \n",
    "\t# Fully connected layers\n",
    "\tmodel.add(layers.Flatten())\n",
    "\tmodel.add(layers.Dense(128, activation='relu'))\n",
    "\tmodel.add(layers.BatchNormalization())\n",
    "\tmodel.add(layers.Dropout(dropout_rate))\n",
    "\tmodel.add(layers.Dense(num_classes, activation='softmax'))\n",
    " \n",
    "\treturn model, vgg19\n",
    "\n",
    "makeVGG19()[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "\n",
    "class UnfreezeAndReduceLRCallback(tf.keras.callbacks.Callback):\n",
    "\tdef __init__(self, base_model, unfreeze_layers=2, reduce_lr_factor=0.5, trigger_epoch=3, layers_unfrozen=0):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.base_model = base_model\n",
    "\t\tself.unfreeze_layers = unfreeze_layers\n",
    "\t\tself.reduce_lr_factor = reduce_lr_factor\n",
    "\t\tself.trigger_epoch = trigger_epoch\n",
    "\t\tself.layers_unfrozen = layers_unfrozen\n",
    "\n",
    "\tdef on_epoch_begin(self, epoch, logs=None):\n",
    "\t\tif epoch == self.trigger_epoch-1:\n",
    "\t\t\ttotal_layers = len(self.base_model.layers)\n",
    "\t\t\tfor layer in self.base_model.layers[-self.unfreeze_layers + self.layers_unfrozen:]:\n",
    "\t\t\t\tlayer.trainable = True\n",
    "\t\t\tprint(f\"Unfroze the last {self.unfreeze_layers + self.layers_unfrozen} layers out of {total_layers} layers.\")\n",
    "\t\t\tself.layers_unfrozen += self.unfreeze_layers\n",
    "\n",
    "\t\t\tcurrent_lr = self.model.optimizer.learning_rate.numpy()\n",
    "\t\t\tnew_lr = current_lr * self.reduce_lr_factor\n",
    "\t\t\ttf.keras.backend.set_value(self.model.optimizer.learning_rate, new_lr)\n",
    "\t\t\tprint(f\"Reduced learning rate from {current_lr:.6f} to {new_lr:.6f}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining k-fold cross validation\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "def RunKFold(modelFunction, optimiser_class, loss_function, callbacks):\n",
    "\t# Create a KFold object\n",
    "\tkf = KFold(n_splits=K, shuffle=True)\n",
    "\n",
    "\tindices = np.arange(len(train_dataset))\n",
    "\n",
    "\tbest_model = None\n",
    "\tbest_val_loss = float('inf')\n",
    "\n",
    "\t# Iterate over K folds\n",
    "\tfor fold, (train_index, val_index) in enumerate(kf.split(indices)):\n",
    "\t\tprint(f\"==================== Fold: {fold+1} ====================\")\n",
    "\t\ttrain = train_dataset.skip(train_index[0]).take(1)\n",
    "\t\tfor index in train_index[1:]:\n",
    "\t\t\ttrain = ((train_dataset.skip(index)).take(1)).concatenate(train)\n",
    "\t\ttrain.cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\t\tval = train_dataset.skip(val_index[0]).take(1)\n",
    "\t\tfor index in val_index[1:]:\n",
    "\t\t\tval = ((train_dataset.skip(index)).take(1)).concatenate(val)\n",
    "\t\tval.cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "  \t\t# Recreate the model to avoid reusing weights\n",
    "\t\tmodel, baseModel = modelFunction(input_shape=(RESIZED_SIZE, RESIZED_SIZE, 1), num_classes=len(class_names))\n",
    "\n",
    "\t\toptimiser = optimiser_class(learning_rate=LEARNING_RATE)\n",
    "\n",
    "\t\t# Compile the model\n",
    "\t\tmodel.compile(optimizer=optimiser, loss=loss_function, metrics=['accuracy'])\n",
    "\n",
    "\t\tunfreezeAndReduceLRCallback = UnfreezeAndReduceLRCallback(baseModel)\n",
    "\t\tcallbacks.append(unfreezeAndReduceLRCallback)\n",
    "  \n",
    "\t\t# Train the model on the training dataset\n",
    "\t\thistory = model.fit(train, epochs=EPOCHS, validation_data=val, callbacks=callbacks)\n",
    "\t\tval_loss = min(model.history.history['val_loss'])\n",
    "  \n",
    "\t\t# Update the best model if this fold is better\n",
    "\t\tif val_loss < best_val_loss:\n",
    "\t\t\tbest_val_loss = val_loss\n",
    "\t\t\tbest_model = model\n",
    "\t\t\tmodel.history_data = history.history\n",
    "\n",
    "\treturn best_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Models with k-fold cross validation\n",
    "from tensorflow.keras import optimizers, losses, callbacks\n",
    "\n",
    "train_dataset.cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "ModelFunctions = [makeInceptionV3, makeVGG19, makeResNet50]\n",
    "ModelNames = [\"InceptionV3\", \"VGG19\", \"ResNet50\"]\n",
    "\n",
    "callbacks = [early_stopping]\n",
    "\n",
    "bestModels = []\n",
    "for model in ModelFunctions:\n",
    "\toptimiser = optimizers.Adam\n",
    "\tloss_function = losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\tprint(f\"============================================== {ModelNames[len(bestModels)]} Model ==============================================\")\n",
    "\tbestModels.append(RunKFold(model, optimiser, loss_function, callbacks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_models(models, test_dataset, class_names):\n",
    "    for i, model in enumerate(models):\n",
    "        print(f\"================ Metrics for {ModelNames[i]} Model ================\")\n",
    "\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        for images, labels in test_dataset:\n",
    "            y_true.extend(labels.numpy())\n",
    "            predictions = model.predict(images)\n",
    "            y_pred.extend(np.argmax(predictions, axis=1))\n",
    "\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title(f\"Confusion Matrix for Model {i+1}\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.show()\n",
    "\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, zero_division=0)\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        metrics = [precision, recall, f1]\n",
    "        metric_names = [\"Precision\", \"Recall\", \"F1-Score\"]\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 4), sharey=True)\n",
    "        for ax, metric, name in zip(axes, metrics, metric_names):\n",
    "            ax.bar(class_names, metric, color='skyblue')\n",
    "            ax.set_title(name)\n",
    "            ax.set_ylim(0, 1)\n",
    "            ax.set_xlabel(\"Class\")\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "        axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(\"======================================================\\n\")\n",
    "\n",
    "evaluate_models(bestModels, test_dataset, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare metrics\n",
    "# Calculate and display metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def plot_training_history(history, model_name):\n",
    "    \"\"\"Plots accuracy and loss graphs from the training history.\"\"\"\n",
    "    epochs = range(1, len(history['accuracy']) + 1)\n",
    "\n",
    "    # Accuracy\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history['accuracy'], label='Training Accuracy', color='blue')\n",
    "    plt.plot(epochs, history['val_accuracy'], label='Validation Accuracy', color='orange')\n",
    "    plt.title(f'{model_name} Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    # Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history['loss'], label='Training Loss', color='blue')\n",
    "    plt.plot(epochs, history['val_loss'], label='Validation Loss', color='orange')\n",
    "    plt.title(f'{model_name} Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_models(models, test_dataset, class_names):\n",
    "    for i, model in enumerate(models):\n",
    "        print(f\"================ Metrics for {ModelNames[i]} Model ================\")\n",
    "\n",
    "        # Plot training history\n",
    "        plot_training_history(model.history_data, ModelNames[i])\n",
    "\n",
    "        # Collect predictions\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        for images, labels in test_dataset:\n",
    "            y_true.extend(labels.numpy())\n",
    "            predictions = model.predict(images)\n",
    "            y_pred.extend(np.argmax(predictions, axis=1))\n",
    "\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "\n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title(f\"Confusion Matrix for {ModelNames[i]}\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.show()\n",
    "\n",
    "        # Precision, recall, F1 score\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, zero_division=0)\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # Bar charts for metrics\n",
    "        metrics = [precision, recall, f1]\n",
    "        metric_names = [\"Precision\", \"Recall\", \"F1-Score\"]\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 4), sharey=True)\n",
    "        for ax, metric, name in zip(axes, metrics, metric_names):\n",
    "            ax.bar(class_names, metric, color='skyblue')\n",
    "            ax.set_title(name)\n",
    "            ax.set_ylim(0, 1)\n",
    "            ax.set_xlabel(\"Class\")\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "        axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(\"======================================================\\n\")\n",
    "\n",
    "# Example call\n",
    "evaluate_models(bestModels, test_dataset, class_names)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
