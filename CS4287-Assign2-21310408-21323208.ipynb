{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of two CNN\n",
    "- Tadhg Ryan 21310408\n",
    "- Szymon Szulc 21323208\n",
    "\n",
    "##### Code executes to end with no errors\n",
    "\n",
    "## Resources:\n",
    "- "
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T16:26:22.178353Z",
     "start_time": "2024-11-19T16:26:21.121715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU devices:\", tf.config.list_physical_devices('GPU'))"
   ],
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _multiarray_umath: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;31mImportError\u001B[0m: DLL load failed while importing _multiarray_umath: The specified module could not be found."
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _multiarray_umath: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;31mImportError\u001B[0m: DLL load failed while importing _multiarray_umath: The specified module could not be found."
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core._multiarray_umath failed to import",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;31mImportError\u001B[0m: numpy.core._multiarray_umath failed to import"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.umath failed to import",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;31mImportError\u001B[0m: numpy.core.umath failed to import"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Unable to convert function return value to a Python type! The signature was\n\t() -> handle",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTensorFlow version:\u001B[39m\u001B[38;5;124m\"\u001B[39m, tf\u001B[38;5;241m.\u001B[39m__version__)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGPU devices:\u001B[39m\u001B[38;5;124m\"\u001B[39m, tf\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mlist_physical_devices(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mGPU\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tf_env1\\lib\\site-packages\\tensorflow\\__init__.py:37\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_sys\u001B[39;00m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_typing\u001B[39;00m\n\u001B[1;32m---> 37\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtools\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m module_util \u001B[38;5;28;01mas\u001B[39;00m _module_util\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlazy_loader\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LazyLoader \u001B[38;5;28;01mas\u001B[39;00m _LazyLoader\n\u001B[0;32m     40\u001B[0m \u001B[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tf_env1\\lib\\site-packages\\tensorflow\\python\\__init__.py:42\u001B[0m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01meager\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m context\n\u001B[0;32m     39\u001B[0m \u001B[38;5;66;03m# pylint: enable=wildcard-import\u001B[39;00m\n\u001B[0;32m     40\u001B[0m \n\u001B[0;32m     41\u001B[0m \u001B[38;5;66;03m# Bring in subpackages.\u001B[39;00m\n\u001B[1;32m---> 42\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m data\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m distribute\n\u001B[0;32m     44\u001B[0m \u001B[38;5;66;03m# from tensorflow.python import keras\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tf_env1\\lib\\site-packages\\tensorflow\\python\\data\\__init__.py:21\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"`tf.data.Dataset` API for input pipelines.\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \n\u001B[0;32m     17\u001B[0m \u001B[38;5;124;03mSee [Importing Data](https://tensorflow.org/guide/data) for an overview.\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# pylint: disable=unused-import\u001B[39;00m\n\u001B[1;32m---> 21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m experimental\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataset_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AUTOTUNE\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataset_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dataset\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tf_env1\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py:96\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"Experimental API for building input pipelines.\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \n\u001B[0;32m     17\u001B[0m \u001B[38;5;124;03mThis module contains experimental `Dataset` sources and transformations that can\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     92\u001B[0m \u001B[38;5;124;03m@@UNKNOWN_CARDINALITY\u001B[39;00m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     95\u001B[0m \u001B[38;5;66;03m# pylint: disable=unused-import\u001B[39;00m\n\u001B[1;32m---> 96\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m service\n\u001B[0;32m     97\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbatching\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dense_to_ragged_batch\n\u001B[0;32m     98\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbatching\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dense_to_sparse_batch\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tf_env1\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\service\\__init__.py:419\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# ==============================================================================\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"API for using the tf.data service.\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \n\u001B[0;32m     17\u001B[0m \u001B[38;5;124;03mThis module contains:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    416\u001B[0m \u001B[38;5;124;03m  job of ParameterServerStrategy).\u001B[39;00m\n\u001B[0;32m    417\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m--> 419\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata_service_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m distribute\n\u001B[0;32m    420\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata_service_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m from_dataset_id\n\u001B[0;32m    421\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata_service_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m register_dataset\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tf_env1\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\data_service_ops.py:24\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tf2\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m compat\n\u001B[1;32m---> 24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m compression_ops\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mservice\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _pywrap_server_lib\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mservice\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _pywrap_utils\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tf_env1\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\compression_ops.py:16\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# ==============================================================================\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"Ops for compressing and uncompressing dataset elements.\"\"\"\u001B[39;00m\n\u001B[1;32m---> 16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m structure\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m gen_experimental_dataset_ops \u001B[38;5;28;01mas\u001B[39;00m ged_ops\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompress\u001B[39m(element):\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tf_env1\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:23\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msix\u001B[39;00m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwrapt\u001B[39;00m\n\u001B[1;32m---> 23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m nest\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m composite_tensor\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ops\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tf_env1\\lib\\site-packages\\tensorflow\\python\\data\\util\\nest.py:36\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;124;03m\"\"\"## Functions for working with arbitrarily nested sequences of elements.\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \n\u001B[0;32m     18\u001B[0m \u001B[38;5;124;03mNOTE(mrry): This fork of the `tensorflow.python.util.nest` module\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;124;03m   arrays.\u001B[39;00m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msix\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_six\u001B[39;00m\n\u001B[1;32m---> 36\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m sparse_tensor \u001B[38;5;28;01mas\u001B[39;00m _sparse_tensor\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _pywrap_utils\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m nest\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tf_env1\\lib\\site-packages\\tensorflow\\python\\framework\\sparse_tensor.py:24\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tf2\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m composite_tensor\n\u001B[1;32m---> 24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m constant_op\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dtypes\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ops\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tf_env1\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:25\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m types_pb2\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01meager\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m context\n\u001B[1;32m---> 25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01meager\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m execute\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dtypes\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m op_callbacks\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tf_env1\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:23\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pywrap_tfe\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01meager\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m core\n\u001B[1;32m---> 23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dtypes\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ops\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tensor_shape\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tf_env1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:34\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtypes\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m trace\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunction\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m trace_type\n\u001B[1;32m---> 34\u001B[0m _np_bfloat16 \u001B[38;5;241m=\u001B[39m \u001B[43m_pywrap_bfloat16\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTF_bfloat16_type\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mDTypeMeta\u001B[39;00m(\u001B[38;5;28mtype\u001B[39m(_dtypes\u001B[38;5;241m.\u001B[39mDType), abc\u001B[38;5;241m.\u001B[39mABCMeta):\n\u001B[0;32m     38\u001B[0m   \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "\u001B[1;31mTypeError\u001B[0m: Unable to convert function return value to a Python type! The signature was\n\t() -> handle"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import dataset\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "dataset_path = kagglehub.dataset_download(\"muratkokludataset/rice-image-dataset\") + \"\\\\Rice_Image_Dataset\"\n",
    "\n",
    "print(\"Path to dataset files:\", dataset_path)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "IMG_HEIGHT = 250\n",
    "IMG_WIDTH = 250\n",
    "K = 3\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "MAX_SIZE_DATASET = 1000\n",
    "NUM_OF_BATCHES = MAX_SIZE_DATASET // BATCH_SIZE\n",
    "RESIZED_SIZE = 128\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load data in\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create the full dataset (without splitting for validation)\n",
    "train_dataset, test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    color_mode='grayscale',  # Load labels in grayscale\n",
    "    shuffle=True,\n",
    "    validation_split=0.004,\n",
    "    subset=\"both\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Get class names for later use\n",
    "class_names = train_dataset.class_names\n",
    "\n",
    "train_dataset = train_dataset.take(NUM_OF_BATCHES)\n",
    "print(f\"Taking {NUM_OF_BATCHES} batches\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define the data augmentation pipeline\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\"), \t\t# Flip horizontally\n",
    "    layers.RandomZoom(-0.2, 0.2),        \t\t\t\t# Zoom in on the image\n",
    "    layers.RandomRotation(0.2),          \t\t\t\t# Randomly rotate image\n",
    "    layers.RandomBrightness(factor=(-0.2, 0.2)),\n",
    "])\n",
    "data_scaling = tf.keras.Sequential([\n",
    "    layers.Resizing(RESIZED_SIZE, RESIZED_SIZE),\t\t# Resize to desired dimensions\n",
    "])\n",
    "data_normalisation = tf.keras.Sequential([\n",
    "    layers.Rescaling(1./255),\n",
    "])\n",
    "\n",
    "# Define functions to apply the augmentation\n",
    "def augment_image(image, label):\n",
    "    image = data_augmentation(image, training=True)\n",
    "    return image, label\n",
    "\n",
    "def scale_image(image, label):\n",
    "    image = data_scaling(image, training=True)\n",
    "    return image, label\n",
    "\n",
    "def normalise_image(image, label):\n",
    "    image = data_normalisation(image)\n",
    "    return image, label\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "num_images = 9  # Number of images to display\n",
    "\n",
    "# Create lists to store images and augmented images\n",
    "original_images = []\n",
    "augmented_images = []\n",
    "labels_list = []\n",
    "\n",
    "# Iterate through the dataset and collect images and their augmented versions\n",
    "i = 0\n",
    "for image_batch, label_batch in train_dataset:\n",
    "    if i >= num_images:\n",
    "        break\n",
    "    original_images.append(image_batch)  # Save original image\n",
    "    augmented_image = augment_image(image_batch, None)[0]\n",
    "    augmented_image = scale_image(augmented_image, None)[0]\n",
    "    augmented_image = normalise_image(augmented_image, None)[0]\n",
    "    augmented_images.append(augmented_image)  # Save augmented image\n",
    "    labels_list.append(label_batch)\n",
    "    i += BATCH_SIZE\n",
    "\n",
    "# Concatenate the batches into single arrays\n",
    "original_images_array = tf.concat(original_images, axis=0)\n",
    "augmented_images_array = tf.concat(augmented_images, axis=0)\n",
    "labels_array = tf.concat(labels_list, axis=0)\n",
    "\n",
    "# Select the first `num_images` images and labels to display\n",
    "original_images_to_display = original_images_array[:num_images]\n",
    "augmented_images_to_display = augmented_images_array[:num_images]\n",
    "labels_to_display = labels_array[:num_images]\n",
    "\n",
    "# Plot the images in a grid\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "for i in range(num_images):\n",
    "    # Plot original images\n",
    "    ax = plt.subplot(3, 6, 2 * i + 1)  # Adjust for both original and augmented\n",
    "    plt.imshow(original_images_to_display[i].numpy().squeeze(), cmap='gray')\n",
    "    plt.title(f\"Original - {class_names[labels_to_display[i].numpy()]}\")  # Show class name\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Plot augmented images\n",
    "    ax = plt.subplot(3, 6, 2 * i + 2)\n",
    "    plt.imshow(augmented_images_to_display[i].numpy().squeeze(), cmap='gray')\n",
    "    plt.title(f\"Augmented - {class_names[labels_to_display[i].numpy()]}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "augmented_train_dataset         = train_dataset.map(augment_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "scale_augmented_train_dataset   = augmented_train_dataset.map(scale_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_dataset                   = scale_augmented_train_dataset.map(normalise_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "scale_test_dataset              = test_dataset.map(scale_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_dataset                    = scale_test_dataset.map(normalise_image, num_parallel_calls=tf.data.AUTOTUNE)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create Model 1\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def makeInceptionV3(input_shape=(224, 224, 1), num_classes=5, dropout_rate=0.5):\n",
    "\tmodel = models.Sequential([])\n",
    "\tmodel.add(layers.Input(shape=input_shape))\n",
    "\tmodel.add(layers.Resizing(224, 224))\n",
    "\tmodel.add(layers.Lambda(lambda x: tf.repeat(x, 3, axis=-1)))\n",
    "\n",
    "\tinceptionV3 = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "\tfor layer in inceptionV3.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\n",
    "\tmodel.add(inceptionV3)\n",
    " \n",
    "\t# Fully connected layers\n",
    "\tmodel.add(layers.Flatten())\n",
    "\tmodel.add(layers.Dense(128, activation='relu'))\n",
    "\tmodel.add(layers.BatchNormalization())\n",
    "\tmodel.add(layers.Dropout(dropout_rate))\n",
    "\tmodel.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\treturn model, inceptionV3\n",
    "\n",
    "makeInceptionV3()[0].summary()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define ResNet50\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "\n",
    "def makeResNet50(input_shape=(224, 224, 1), num_classes=5, dropout_rate=0.5):\n",
    "    base_model = ResNet50(\n",
    "        include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3)\n",
    "    )\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(input_shape))\n",
    "    model.add(layers.Resizing(224, 224))\n",
    "    model.add(layers.Lambda(lambda x: tf.repeat(x, 3, axis=-1)))\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.add(base_model)\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation=\"relu\"))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "    return model, base_model\n",
    "\n",
    "makeResNet50()[0].summary()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# VGG16 define\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import VGG19\n",
    "\n",
    "def makeVGG19(input_shape=(224, 224, 1), num_classes=5, dropout_rate=0.5):\n",
    "\tmodel = models.Sequential()\n",
    "\n",
    "\tmodel.add(layers.Input(input_shape))\n",
    "\tmodel.add(layers.Resizing(224, 224))\n",
    "\tmodel.add(layers.Lambda(lambda x: tf.repeat(x, 3, axis=-1)))\n",
    "\n",
    "\tvgg19 = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "\tfor layer in vgg19.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\n",
    "\tmodel.add(vgg19)\n",
    " \n",
    "\t# Fully connected layers\n",
    "\tmodel.add(layers.Flatten())\n",
    "\tmodel.add(layers.Dense(128, activation='relu'))\n",
    "\tmodel.add(layers.BatchNormalization())\n",
    "\tmodel.add(layers.Dropout(dropout_rate))\n",
    "\tmodel.add(layers.Dense(num_classes, activation='softmax'))\n",
    " \n",
    "\treturn model, vgg19\n",
    "\n",
    "makeVGG19()[0].summary()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Callbacks\n",
    "\n",
    "class UnfreezeAndReduceLRCallback(tf.keras.callbacks.Callback):\n",
    "\tdef __init__(self, base_model, unfreeze_layers=2, reduce_lr_factor=0.5, trigger_epoch=3, layers_unfrozen=0):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.base_model = base_model\n",
    "\t\tself.unfreeze_layers = unfreeze_layers\n",
    "\t\tself.reduce_lr_factor = reduce_lr_factor\n",
    "\t\tself.trigger_epoch = trigger_epoch\n",
    "\t\tself.layers_unfrozen = layers_unfrozen\n",
    "\n",
    "\tdef on_epoch_begin(self, epoch, logs=None):\n",
    "\t\tif epoch == self.trigger_epoch-1:\n",
    "\t\t\ttotal_layers = len(self.base_model.layers)\n",
    "\t\t\tfor layer in self.base_model.layers[-self.unfreeze_layers + self.layers_unfrozen:]:\n",
    "\t\t\t\tlayer.trainable = True\n",
    "\t\t\tprint(f\"Unfroze the last {self.unfreeze_layers + self.layers_unfrozen} layers out of {total_layers} layers.\")\n",
    "\t\t\tself.layers_unfrozen += self.unfreeze_layers\n",
    "\n",
    "\t\t\tcurrent_lr = self.model.optimizer.learning_rate.numpy()\n",
    "\t\t\tnew_lr = current_lr * self.reduce_lr_factor\n",
    "\t\t\ttf.keras.backend.set_value(self.model.optimizer.learning_rate, new_lr)\n",
    "\t\t\tprint(f\"Reduced learning rate from {current_lr:.4f} to {new_lr:.4f}.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Defining k-fold cross validation\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "def RunKFold(modelFunction, optimiser_class, loss_function, callbacks):\n",
    "\t# Create a KFold object\n",
    "\tkf = KFold(n_splits=K, shuffle=True)\n",
    "\n",
    "\tindices = np.arange(len(train_dataset))\n",
    "\n",
    "\tbest_model = None\n",
    "\tbest_val_loss = float('inf')\n",
    "\n",
    "\t# Iterate over K folds\n",
    "\tfor fold, (train_index, val_index) in enumerate(kf.split(indices)):\n",
    "\t\tprint(f\"==================== Fold: {fold+1} ====================\")\n",
    "\t\ttrain = train_dataset.skip(train_index[0]).take(1)\n",
    "\t\tfor index in train_index[1:]:\n",
    "\t\t\ttrain = ((train_dataset.skip(index)).take(1)).concatenate(train)\n",
    "\t\ttrain.cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\t\tval = train_dataset.skip(val_index[0]).take(1)\n",
    "\t\tfor index in val_index[1:]:\n",
    "\t\t\tval = ((train_dataset.skip(index)).take(1)).concatenate(val)\n",
    "\t\tval.cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "  \t\t# Recreate the model to avoid reusing weights\n",
    "\t\tmodel, baseModel = modelFunction(input_shape=(RESIZED_SIZE, RESIZED_SIZE, 1), num_classes=len(class_names))\n",
    "\n",
    "\t\toptimiser = optimiser_class(learning_rate=LEARNING_RATE)\n",
    "\n",
    "\t\t# Compile the model\n",
    "\t\tmodel.compile(optimizer=optimiser, loss=loss_function, metrics=['accuracy'])\n",
    "\n",
    "\t\tunfreezeAndReduceLRCallback = UnfreezeAndReduceLRCallback(baseModel)\n",
    "\t\tcallbacks.append(unfreezeAndReduceLRCallback)\n",
    "  \n",
    "\t\t# Train the model on the training dataset\n",
    "\t\thistory = model.fit(train, epochs=EPOCHS, validation_data=val, callbacks=callbacks)\n",
    "\t\tval_loss = min(model.history.history['val_loss'])\n",
    "  \n",
    "\t\t# Update the best model if this fold is better\n",
    "\t\tif val_loss < best_val_loss:\n",
    "\t\t\tbest_val_loss = val_loss\n",
    "\t\t\tbest_model = model\n",
    "\t\t\tmodel.history_data = history.history\n",
    "\n",
    "\treturn best_model\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run Models with k-fold cross validation\n",
    "from tensorflow.keras import optimizers, losses, callbacks\n",
    "\n",
    "train_dataset.cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "ModelFunctions = [makeInceptionV3, makeVGG19, makeResNet50]\n",
    "ModelNames = [\"InceptionV3\", \"VGG19\", \"ResNet50\"]\n",
    "\n",
    "callbacks = [early_stopping]\n",
    "\n",
    "bestModels = []\n",
    "for model in ModelFunctions:\n",
    "\toptimiser = optimizers.Adam\n",
    "\tloss_function = losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\tprint(f\"============================================== {ModelNames[len(bestModels)]} Model ==============================================\")\n",
    "\tbestModels.append(RunKFold(model, optimiser, loss_function, callbacks))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calculate and display metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_models(models, test_dataset, class_names):\n",
    "    for i, model in enumerate(models):\n",
    "        print(f\"================ Metrics for {ModelNames[i]} Model ================\")\n",
    "\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        for images, labels in test_dataset:\n",
    "            y_true.extend(labels.numpy())\n",
    "            predictions = model.predict(images)\n",
    "            y_pred.extend(np.argmax(predictions, axis=1))\n",
    "\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title(f\"Confusion Matrix for Model {i+1}\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.show()\n",
    "\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, zero_division=0)\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        metrics = [precision, recall, f1]\n",
    "        metric_names = [\"Precision\", \"Recall\", \"F1-Score\"]\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 4), sharey=True)\n",
    "        for ax, metric, name in zip(axes, metrics, metric_names):\n",
    "            ax.bar(class_names, metric, color='skyblue')\n",
    "            ax.set_title(name)\n",
    "            ax.set_ylim(0, 1)\n",
    "            ax.set_xlabel(\"Class\")\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "        axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(\"======================================================\\n\")\n",
    "\n",
    "evaluate_models(bestModels, test_dataset, class_names)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compare metrics\n",
    "# Calculate and display metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def plot_training_history(history, model_name):\n",
    "    \"\"\"Plots accuracy and loss graphs from the training history.\"\"\"\n",
    "    epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "\n",
    "    # Accuracy\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history.history['accuracy'], label='Training Accuracy', color='blue')\n",
    "    plt.plot(epochs, history.history['val_accuracy'], label='Validation Accuracy', color='orange')\n",
    "    plt.title(f'{model_name} Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    # Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history.history['loss'], label='Training Loss', color='blue')\n",
    "    plt.plot(epochs, history.history['val_loss'], label='Validation Loss', color='orange')\n",
    "    plt.title(f'{model_name} Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_models(models, test_dataset, class_names, histories):\n",
    "    for i, model in enumerate(models):\n",
    "        print(f\"================ Metrics for {ModelNames[i]} Model ================\")\n",
    "\n",
    "        # Plot training history\n",
    "        plot_training_history(histories[i], ModelNames[i])\n",
    "\n",
    "        # Collect predictions\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        for images, labels in test_dataset:\n",
    "            y_true.extend(labels.numpy())\n",
    "            predictions = model.predict(images)\n",
    "            y_pred.extend(np.argmax(predictions, axis=1))\n",
    "\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "\n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title(f\"Confusion Matrix for {ModelNames[i]}\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.show()\n",
    "\n",
    "        # Precision, recall, F1 score\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, zero_division=0)\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # Bar charts for metrics\n",
    "        metrics = [precision, recall, f1]\n",
    "        metric_names = [\"Precision\", \"Recall\", \"F1-Score\"]\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 4), sharey=True)\n",
    "        for ax, metric, name in zip(axes, metrics, metric_names):\n",
    "            ax.bar(class_names, metric, color='skyblue')\n",
    "            ax.set_title(name)\n",
    "            ax.set_ylim(0, 1)\n",
    "            ax.set_xlabel(\"Class\")\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "        axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(\"======================================================\\n\")\n",
    "\n",
    "# Example call\n",
    "evaluate_models(bestModels, test_dataset, class_names, histories)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
